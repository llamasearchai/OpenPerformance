# ML Performance Engineering Platform

A comprehensive platform for optimizing and monitoring machine learning workloads across distributed systems. Below is the complete production-ready implementation with all components.

## Project Structure

```
# python/mlperf/optimization/distributed.py
import os
import time
import json
import logging
import socket
import threading
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass, field
import numpy as np

from ..utils.logging import get_logger
from ..hardware.gpu import GPUInfo, get_gpu_info
# Add OpenAI import and API key loading utility
import openai
from ..utils.config import get_openai_api_key # Assuming config.py is in utils

logger = get_logger(__name__)

# Helper class for OpenAI interactions
class OpenAIHelper:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or get_openai_api_key()
        if not self.api_key:
            logger.warning("OpenAI API key not provided. AI-powered recommendations will be disabled.")
            self.client = None
        else:
            try:
                self.client = openai.OpenAI(api_key=self.api_key)
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                self.client = None

    def generate_recommendations(self, bottlenecks: List[Dict[str, Any]], category_times: Dict[str, float], top_events: List[Tuple[str, float]], total_runtime: float) -> List[str]:
        if not self.client:
            return ["OpenAI client not initialized. Cannot generate AI recommendations."]

        prompt = self._construct_prompt(bottlenecks, category_times, top_events, total_runtime)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",  # Or a more advanced model if preferred
                messages=[
                    {"role": "system", "content": "You are an expert in ML performance engineering. Provide actionable recommendations based on the following profiling data."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.5,
            )
            ai_recommendations = response.choices[0].message.content.strip().split('\n')
            return [rec.strip() for rec in ai_recommendations if rec.strip()]
        except Exception as e:
            logger.error(f"Error calling OpenAI API: {e}")
            return [f"Error generating AI recommendations: {e}"]

    def _construct_prompt(
        self,
        bottlenecks: List[Dict[str, Any]],
        category_times: Dict[str, float],
        top_events: List[Tuple[str, float]],
        total_runtime: float
    ) -> str:
        prompt = "Profiling Analysis:\n"
        prompt += f"- Total Runtime: {total_runtime:.2f}s\n"
        prompt += "- Identified Bottlenecks:\n"
        for bottleneck in bottlenecks:
            prompt += f"  - Type: {bottleneck['type']}, Name: {bottleneck.get('name', 'N/A')}, Percentage: {bottleneck.get('percentage', 0):.1f}%\n"
        prompt += "- Top Time-Consuming Events:\n"
        for name, duration in top_events[:5]:
            prompt += f"  - Event: {name}, Duration: {duration:.4f}s, Percentage: {(duration / total_runtime) * 100:.1f}%\n"
        prompt += "- Time Distribution by Category:\n"
        for category, time in category_times.items():
            prompt += f"  - Category: {category}, Time: {time:.4f}s, Percentage: {(time / total_runtime) * 100:.1f}%\n"
        prompt += "\nPlease provide actionable recommendations to optimize performance based on this data."
        return prompt

@dataclass
class NodeInfo:
    """Information about a node in the distributed cluster."""
    hostname: str
    ip_address: str
    rank: int
    local_rank: int
    world_size: int
    gpus: List[GPUInfo] = field(default_factory=list)
    cpu_cores: int = 0
    memory_gb: float = 0
    network_bandwidth_gbps: float = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "hostname": self.hostname,
            "ip_address": self.ip_address,
            "rank": self.rank,
            "local_rank": self.local_rank,
            "world_size": self.world_size,
            "gpus": [gpu.to_dict() for gpu in self.gpus],
            "cpu_cores": self.cpu_cores,
            "memory_gb": self.memory_gb,
            "network_bandwidth_gbps": self.network_bandwidth_gbps
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'NodeInfo':
        """Create from dictionary."""
        return cls(
            hostname=data["hostname"],
            ip_address=data["ip_address"],
            rank=data["rank"],
            local_rank=data["local_rank"],
            world_size=data["world_size"],
            gpus=[GPUInfo.from_dict(gpu) for gpu in data.get("gpus", [])],
            cpu_cores=data.get("cpu_cores", 0),
            memory_gb=data.get("memory_gb", 0),
            network_bandwidth_gbps=data.get("network_bandwidth_gbps", 0)
        )
    
    @classmethod
    def get_current(cls, rank: int, local_rank: int, world_size: int) -> 'NodeInfo':
        """Get information about the current node."""
        hostname = socket.gethostname()
        try:
            ip_address = socket.gethostbyname(hostname)
        except socket.gaierror:
            ip_address = "127.0.0.1"
        
        # Get GPU information
        gpus = get_gpu_info()
        
        # Get CPU and memory information
        import psutil
        cpu_cores = os.cpu_count() or 0
        memory_gb = psutil.virtual_memory().total / (1024 ** 3)
        
        # Estimate network bandwidth (just a placeholder - would need actual measurement)
        network_bandwidth_gbps = 10.0  # Placeholder value
        
        return cls(
            hostname=hostname,
            ip_address=ip_address,
            rank=rank,
            local_rank=local_rank,
            world_size=world_size,
            gpus=gpus,
            cpu_cores=cpu_cores,
            memory_gb=memory_gb,
            network_bandwidth_gbps=network_bandwidth_gbps
        )

@dataclass
class CommunicationConfig:
    """Configuration for optimizing distributed communication."""
    backend: str = "nccl"  # nccl, gloo, mpi
    bucket_size_mb: int = 25
    gradient_compression: bool = False
    compression_ratio: float = 0.01  # For algorithms like PowerSGD
    allreduce_always_fp16: bool = False
    optimize_network_topology: bool = True
    enable_mixed_precision: bool = True
    enable_zero_redundancy: bool = False
    zero_stage: int = 1  # 1, 2, or 3 for ZeRO stages
    overlap_comm_comp: bool = True
    num_threads: int = 4

class DistributedOptimizer:
    """Optimizes distributed training performance."""
    
    def __init__(
        self,
        config: CommunicationConfig,
        framework: str = "pytorch",
        profile_first_step: bool = True
    ):
        self.config = config
        self.framework = framework.lower()
        self.profile_first_step = profile_first_step
        self.node_info = None
        self.cluster_info = []
        self.initialized = False
        
        # Load rust performance monitoring module
        try:
            from mlperf_rust import profiling as rust_profiling
            self.rust_profiling = rust_profiling
            logger.info("Loaded rust performance profiling module")
        except ImportError:
            logger.warning("Rust profiling module not available")
            self.rust_profiling = None
    
    def initialize(
        self,
        rank: int,
        local_rank: int,
        world_size: int,
        master_addr: Optional[str] = None,
        master_port: int = 29500
    ) -> None:
        """Initialize distributed training environment."""
        # Get information about the current node
        self.node_info = NodeInfo.get_current(rank, local_rank, world_size)
        
        # Initialize framework-specific distributed environment
        if self.framework == "pytorch":
            self._init_pytorch(rank, world_size, master_addr, master_port)
        elif self.framework == "tensorflow":
            self._init_tensorflow(rank, world_size)
        elif self.framework == "jax":
            self._init_jax(rank, world_size)
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")
        
        # Start performance monitoring
        if self.rust_profiling:
            self.rust_profiling.start_monitoring(rank, local_rank, world_size)
        
        self.initialized = True
        logger.info(f"Initialized distributed training for rank {rank}/{world_size}")
    
    def _init_pytorch(self, rank: int, world_size: int, master_addr: Optional[str], master_port: int) -> None:
        """Initialize PyTorch distributed environment."""
        try:
            import torch.distributed as dist
            import torch
            
            # Set environment variables if not already set
            os.environ["RANK"] = str(rank)
            os.environ["WORLD_SIZE"] = str(world_size)
            if master_addr:
                os.environ["MASTER_ADDR"] = master_addr
            os.environ["MASTER_PORT"] = str(master_port)
            
            # Initialize distributed process group
            backend = self.config.backend
            if not dist.is_initialized():
                dist.init_process_group(
                    backend=backend,
                    rank=rank,
                    world_size=world_size
                )
            
            # Set optimization parameters
            if self.config.allreduce_always_fp16:
                os.environ["TORCH_DISTRIBUTED_ALLREDUCE_ALWAYS_FP16"] = "1"
            
            if self.config.bucket_size_mb > 0:
                torch.distributed.all_reduce_bucket_size_mb = self.config.bucket_size_mb
            
            # Set NCCL optimization environment variables
            if backend == "nccl":
                os.environ["NCCL_DEBUG"] = "INFO"
                os.environ["NCCL_SOCKET_IFNAME"] = "^lo"
                if self.config.optimize_network_topology:
                    os.environ["NCCL_IB_DISABLE"] = "0"
                    os.environ["NCCL_IB_GID_INDEX"] = "3"
                    os.environ["NCCL_IB_HCA"] = "^mlx5_0"
                    os.environ["NCCL_NET_GDR_LEVEL"] = "5"
            
            logger.info(f"PyTorch distributed initialized with backend {backend}")
            
        except ImportError:
            logger.error("PyTorch not installed")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize PyTorch distributed: {e}")
            raise
    
    def _init_tensorflow(self, rank: int, world_size: int) -> None:
        """Initialize TensorFlow distributed environment."""
        try:
            import tensorflow as tf
            
            # Set multi-threading parameters
            if self.config.num_threads > 0:
                tf.config.threading.set_intra_op_parallelism_threads(self.config.num_threads)
                tf.config.threading.set_inter_op_parallelism_threads(self.config.num_threads)
            
            # Initialize distributed strategy
            if world_size > 1:
                os.environ["TF_CONFIG"] = json.dumps({
                    "cluster": {
                        "worker": [f"localhost:{12345 + i}" for i in range(world_size)]
                    },
                    "task": {"type": "worker", "index": rank}
                })
                
                strategy = tf.distribute.MultiWorkerMirroredStrategy()
                logger.info(f"TensorFlow distributed initialized with {world_size} workers")
            else:
                strategy = tf.distribute.MirroredStrategy()
                logger.info("TensorFlow single-node multi-GPU strategy initialized")
            
            # Store strategy for later use
            self.strategy = strategy
            
        except ImportError:
            logger.error("TensorFlow not installed")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize TensorFlow distributed: {e}")
            raise
    
    def _init_jax(self, rank: int, world_size: int) -> None:
        """Initialize JAX distributed environment."""
        try:
            import jax
            
            # Configure JAX to use all available devices
            jax.config.update('jax_platforms', 'cpu,gpu,tpu')
            devices = jax.devices()
            logger.info(f"JAX using {len(devices)} devices: {jax.devices()}")
            
            # Set process ID for communication
            if world_size > 1:
                jax.distributed.initialize()
                logger.info(f"JAX distributed initialized for rank {rank}/{world_size}")
            
        except ImportError:
            logger.error("JAX not installed")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize JAX distributed: {e}")
            raise
    
    def optimize_model_parallel(self, model_size_gb: float, num_gpus: int, device_memory_gb: float) -> Tuple[int, int]:
        """
        Optimize model parallelism configuration.
        
        Args:
            model_size_gb: Size of the model in GB
            num_gpus: Number of available GPUs
            device_memory_gb: Available memory per GPU in GB
            
        Returns:
            Tuple of (tensor_parallel_size, pipeline_parallel_size)
        """
        if model_size_gb <= device_memory_gb:
            # Model fits on a single GPU, no need for model parallelism
            return 1, 1
        
        # Simple heuristic for tensor vs pipeline parallelism
        # In practice, this would be much more sophisticated
        memory_per_gpu = device_memory_gb * 0.8  # Leave some headroom
        tensor_parallel_size = min(num_gpus, max(1, int(np.ceil(model_size_gb / memory_per_gpu))))
        
        remaining_gpus = num_gpus // tensor_parallel_size
        pipeline_parallel_size = min(4, remaining_gpus)  # Cap pipeline stages to avoid excessive overhead
        
        logger.info(f"Model parallelism configuration: TP={tensor_parallel_size}, PP={pipeline_parallel_size}")
        return tensor_parallel_size, pipeline_parallel_size
    
    def optimize_data_parallel(
        self, 
        batch_size: int, 
        model_size_gb: float,
        num_gpus: int,
        global_batch_size: Optional[int] = None,
        min_batch_per_gpu: int = 1
    ) -> Tuple[int, int]:
        """
        Optimize data parallelism configuration.
        
        Args:
            batch_size: Per-GPU batch size (starting point)
            model_size_gb: Size of the model in GB
            num_gpus: Number of available GPUs
            global_batch_size: Target global batch size (optional)
            min_batch_per_gpu: Minimum batch size per GPU
            
        Returns:
            Tuple of (optimal_batch_size_per_gpu, gradient_accumulation_steps)
        """
        if global_batch_size is None:
            # If no global batch size specified, use current batch size * num_gpus
            global_batch_size = batch_size * num_gpus
        
        # Adjust for model size - larger models need more gradient accumulation
        grad_accum_factor = max(1, int(model_size_gb / 10))  # Simple heuristic
        
        # Calculate batch size per GPU and gradient accumulation steps
        batch_size_per_gpu = max(min_batch_per_gpu, global_batch_size // num_gpus)
        gradient_accumulation_steps = max(1, global_batch_size // (batch_size_per_gpu * num_gpus))
        
        # Apply gradient accumulation factor for large models
        gradient_accumulation_steps *= grad_accum_factor
        
        logger.info(f"Data parallelism configuration: batch_size_per_gpu={batch_size_per_gpu}, "
                   f"gradient_accumulation_steps={gradient_accumulation_steps}, "
                   f"global_batch_size={batch_size_per_gpu * num_gpus * gradient_accumulation_steps}")
        
        return batch_size_per_gpu, gradient_accumulation_steps
    
    def optimize_communication(self, model_size_gb: float, num_parameters: int, world_size: int) -> Dict[str, Any]:
        """
        Optimize communication settings for distributed training.
        
        Args:
            model_size_gb: Size of the model in GB
            num_parameters: Number of model parameters
            world_size: Number of processes in the distributed training
            
        Returns:
            Dictionary of optimized communication settings
        """
        # Start with current config
        optimized = {
            "backend": self.config.backend,
            "bucket_size_mb": self.config.bucket_size_mb,
            "gradient_compression": self.config.gradient_compression,
            "compression_ratio": self.config.compression_ratio,
            "allreduce_always_fp16": self.config.allreduce_always_fp16,
            "zero_stage": self.config.zero_stage
        }
        
        # Adjust bucket size based on model size and world size
        if model_size_gb > 10:
            # For very large models, use larger buckets to reduce overhead
            optimized["bucket_size_mb"] = min(200, max(25, int(100 * model_size_gb / 40)))
        elif world_size > 16:
            # For large clusters, use smaller buckets for better overlap
            optimized["bucket_size_mb"] = max(5, min(25, int(25 * 16 / world_size)))
        
        # Enable gradient compression for very large models or clusters
        if model_size_gb > 20 or world_size > 32:
            optimized["gradient_compression"] = True
            # Adjust compression ratio based on model size
            if model_size_gb > 40:
                optimized["compression_ratio"] = 0.001  # More aggressive for very large models
            elif model_size_gb > 20:
                optimized["compression_ratio"] = 0.005  # Moderate for large models
            else:
                optimized["compression_ratio"] = 0.01   # Default
        
        # Enable fp16 allreduce for better network utilization
        optimized["allreduce_always_fp16"] = True
        
        # Optimize ZeRO stage based on model size and world size
        if model_size_gb > 100:
            optimized["zero_stage"] = 3  # Full parameter, gradient, and optimizer state partitioning
        elif model_size_gb > 20 or world_size > 16:
            optimized["zero_stage"] = 2  # Parameter and gradient partitioning
        else:
            optimized["zero_stage"] = 1  # Gradient partitioning only
        
        logger.info(f"Optimized communication settings: {optimized}")
        return optimized
    
    def benchmark_allreduce(self, size_mb: float, iterations: int = 10) -> Dict[str, float]:
        """
        Benchmark allreduce performance for a given tensor size.
        
        Args:
            size_mb: Size of the tensor in MB
            iterations: Number of benchmark iterations
            
        Returns:
            Dictionary with benchmark results
        """
        if not self.initialized:
            raise RuntimeError("Distributed optimizer not initialized")
        
        if self.framework == "pytorch":
            return self._benchmark_allreduce_pytorch(size_mb, iterations)
        elif self.framework == "tensorflow":
            return self._benchmark_allreduce_tensorflow(size_mb, iterations)
        else:
            logger.warning(f"Allreduce benchmark not implemented for {self.framework}")
            return {"error": f"Not implemented for {self.framework}"}
    
    def _benchmark_allreduce_pytorch(self, size_mb: float, iterations: int) -> Dict[str, float]:
        """Benchmark allreduce performance for PyTorch."""
        try:
            import torch
            import torch.distributed as dist
            
            if not dist.is_initialized():
                logger.error("PyTorch distributed not initialized")
                return {"error": "Distributed not initialized"}
            
            # Create tensor to allreduce
            size_bytes = int(size_mb * 1024 * 1024)
            num_elements = size_bytes // 4  # assuming float32
            tensor = torch.randn(num_elements, dtype=torch.float32, device="cuda")
            
            # Warmup
            for _ in range(3):
                dist.all_reduce(tensor)
            
            # Synchronize before timing
            torch.cuda.synchronize()
            
            # Benchmark
            start_time = time.time()
            for _ in range(iterations):
                dist.all_reduce(tensor)
                torch.cuda.synchronize()
            end_time = time.time()
            
            elapsed_time = end_time - start_time
            time_per_iter_ms = (elapsed_time / iterations) * 1000
            bandwidth_gb_s = (size_mb / 1024) / (time_per_iter_ms / 1000)
            
            return {
                "size_mb": size_mb,
                "iterations": iterations,
                "total_time_s": elapsed_time,
                "time_per_iter_ms": time_per_iter_ms,
                "bandwidth_gb_s": bandwidth_gb_s
            }
            
        except Exception as e:
            logger.error(f"Error in PyTorch allreduce benchmark: {e}")
            return {"error": str(e)}
    
    def _benchmark_allreduce_tensorflow(self, size_mb: float, iterations: int) -> Dict[str, float]:
        """Benchmark allreduce performance for TensorFlow."""
        try:
            import tensorflow as tf
            import horovod.tensorflow as hvd
            
            # Check if Horovod is initialized
            if not hvd.is_initialized():
                logger.error("Horovod not initialized")
                return {"error": "Horovod not initialized"}
            
            # Create tensor to allreduce
            size_bytes = int(size_mb * 1024 * 1024)
            num_elements = size_bytes // 4  # assuming float32
            tensor = tf.random.normal([num_elements], dtype=tf.float32)
            
            # Warmup
            for _ in range(3):
                hvd.allreduce(tensor)
            
            # Benchmark
            start_time = time.time()
            for _ in range(iterations):
                hvd.allreduce(tensor)
            end_time = time.time()
            
            elapsed_time = end_time - start_time
            time_per_iter_ms = (elapsed_time / iterations) * 1000
            bandwidth_gb_s = (size_mb / 1024) / (time_per_iter_ms / 1000)
            
            return {
                "size_mb": size_mb,
                "iterations": iterations,
                "total_time_s": elapsed_time,
                "time_per_iter_ms": time_per_iter_ms,
                "bandwidth_gb_s": bandwidth_gb_s
            }
            
        except Exception as e:
            logger.error(f"Error in TensorFlow allreduce benchmark: {e}")
            return {"error": str(e)}

    def analyze_collective_performance(self, trace_file: str) -> Dict[str, Any]:
        """
        Analyze collective communication performance from a trace file.
        
        Args:
            trace_file: Path to the trace file
            
        Returns:
            Dictionary with analysis results
        """
        if self.rust_profiling:
            # Use Rust implementation for better performance
            return self.rust_profiling.analyze_collective_performance(trace_file)
        
        # Fallback Python implementation
        try:
            import pandas as pd
            import json
            
            # Load trace file
            with open(trace_file, "r") as f:
                trace_data = json.load(f)
            
            # Extract collective operations
            collectives = []
            for event in trace_data["traceEvents"]:
                if "name" in event and "ph" in event:
                    if event["name"].startswith("nccl:") or event["name"].startswith("allreduce"):
                        collectives.append({
                            "name": event["name"],
                            "ts": event["ts"],
                            "dur": event.get("dur", 0),
                            "pid": event.get("pid", 0),
                            "tid": event.get("tid", 0),
                            "args": event.get("args", {})
                        })
            
            if not collectives:
                return {"error": "No collective operations found in trace"}
            
            # Convert to DataFrame for analysis
            df = pd.DataFrame(collectives)
            
            # Calculate statistics
            stats = {
                "total_ops": len(df),
                "total_duration_ms": df["dur"].sum() / 1000,
                "avg_duration_ms": df["dur"].mean() / 1000,
                "max_duration_ms": df["dur"].max() / 1000,
                "min_duration_ms": df["dur"].min() / 1000,
                "p95_duration_ms": df["dur"].quantile(0.95) / 1000,
                "p99_duration_ms": df["dur"].quantile(0.99) / 1000
            }
            
            # Group by operation name
            op_stats = {}
            for name, group in df.groupby("name"):
                op_stats[name] = {
                    "count": len(group),
                    "total_duration_ms": group["dur"].sum() / 1000,
                    "avg_duration_ms": group["dur"].mean() / 1000,
                    "max_duration_ms": group["dur"].max() / 1000,
                    "min_duration_ms": group["dur"].min() / 1000
                }
            
            return {
                "overall_stats": stats,
                "op_stats": op_stats,
                "num_processes": len(df["pid"].unique()),
                "num_threads": len(df["tid"].unique())
            }
            
        except Exception as e:
            logger.error(f"Error analyzing collective performance: {e}")
            return {"error": str(e)}

@dataclass
class MemoryConfig:
    """Memory optimization configuration."""
    enable_activation_checkpointing: bool = True
    enable_offloading: bool = False
    offload_optimizer_state: bool = False
    offload_parameters: bool = False
    offload_activations: bool = False
    cpu_offload_params: bool = False
    cpu_offload_use_pin_memory: bool = True
    memory_efficient_optimizer: bool = True
    overlap_comm_prepostprocessing: bool = True
    min_params_bucket_size: int = 1e8
    max_reuse_distance_in_numel: int = 1e9
    release_inference_cache: bool = True
    optimize_memory_usage: bool = True
    cudnn_benchmark: bool = True
    use_custom_allocator: bool = False

class MemoryTracker:
    """Tracks memory usage during training and inference."""
    
    def __init__(self, framework: str = "pytorch", interval_ms: int = 100):
        self.framework = framework.lower()
        self.interval_ms = interval_ms
        self.tracking = False
        self.memory_logs = []
        self.tracking_thread = None
        self.stop_event = threading.Event()
        
        # Initialize memory tracker based on framework
        if self.framework == "pytorch" and not TORCH_AVAILABLE:
            raise ImportError("PyTorch not installed but framework set to pytorch")
        elif self.framework == "tensorflow" and not TF_AVAILABLE:
            raise ImportError("TensorFlow not installed but framework set to tensorflow")
    
    def start_tracking(self) -> None:
        """Start tracking memory usage."""
        if self.tracking:
            logger.warning("Memory tracking already started")
            return
        
        self.stop_event.clear()
        self.memory_logs = []
        self.tracking = True
        
        self.tracking_thread = threading.Thread(
            target=self._tracking_loop,
            daemon=True
        )
        self.tracking_thread.start()
        
        logger.info(f"Started memory tracking for {self.framework}")
    
    def stop_tracking(self) -> List[MemoryUsage]:
        """Stop tracking memory usage and return logged data."""
        if not self.tracking:
            logger.warning("Memory tracking not started")
            return self.memory_logs
        
        self.stop_event.set()
        if self.tracking_thread:
            self.tracking_thread.join(timeout=5)
        
        self.tracking = False
        logger.info(f"Stopped memory tracking, collected {len(self.memory_logs)} samples")
        
        return self.memory_logs
    
    def _tracking_loop(self) -> None:
        """Background loop for memory tracking."""
        while not self.stop_event.is_set():
            try:
                memory_usage = self._get_memory_usage()
                self.memory_logs.append(memory_usage)
            except Exception as e:
                logger.error(f"Error in memory tracking: {e}")
            
            # Sleep for the specified interval
            time.sleep(self.interval_ms / 1000)
    
    def _get_memory_usage(self) -> MemoryUsage:
        """Get current memory usage based on framework."""
        if self.framework == "pytorch":
            return self._get_pytorch_memory()
        elif self.framework == "tensorflow":
            return self._get_tensorflow_memory()
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")
    
    def _get_pytorch_memory(self) -> MemoryUsage:
        """Get PyTorch GPU memory usage."""
        import torch
        
        if not torch.cuda.is_available():
            return MemoryUsage(
                timestamp=time.time(),
                device="cpu",
                total_bytes=0,
                used_bytes=0
            )
        
        device = torch.cuda.current_device()
        
        # Get memory statistics
        total_bytes = torch.cuda.get_device_properties(device).total_memory
        used_bytes = torch.cuda.memory_allocated(device)
        reserved_bytes = torch.cuda.memory_reserved(device)
        
        # Get additional statistics if available
        try:
            active_bytes = torch.cuda.memory_stats(device)["active_bytes.all.current"]
            inactive_bytes = reserved_bytes - active_bytes
            
            # Estimate fragmentation
            fragmentation = 0.0
            if reserved_bytes > 0:
                fragmentation = 1.0 - (used_bytes / reserved_bytes)
                
        except (KeyError, RuntimeError):
            active_bytes = 0
            inactive_bytes = 0
            fragmentation = 0.0
        
        return MemoryUsage(
            timestamp=time.time(),
            device=f"cuda:{device}",
            total_bytes=total_bytes,
            used_bytes=used_bytes,
            reserved_bytes=reserved_bytes,
            active_bytes=active_bytes,
            inactive_bytes=inactive_bytes,
            fragmentation=fragmentation
        )
    
    def _get_tensorflow_memory(self) -> MemoryUsage:
        """Get TensorFlow GPU memory usage."""
        import tensorflow as tf
        
        if not tf.config.list_physical_devices('GPU'):
            return MemoryUsage(
                timestamp=time.time(),
                device="cpu",
                total_bytes=0,
                used_bytes=0
            )
        
        # Get available GPUs
        gpus = tf.config.list_physical_devices('GPU')
        device = 0  # Use first GPU by default
        
        # Get memory info using experimental memory_info API
        try:
            mem_info = tf.config.experimental.get_memory_info(f'GPU:{device}')
            used_bytes = mem_info.get('current', 0)
            total_bytes = 0  # TensorFlow doesn't provide total memory easily
            
            # Try to get total memory from device properties
            try:
                device_details = tf.config.experimental.get_device_details(gpus[device])
                if 'memory_limit' in device_details:
                    total_bytes = device_details['memory_limit']
            except:
                pass
            
            return MemoryUsage(
                timestamp=time.time(),
                device=f"GPU:{device}",
                total_bytes=total_bytes,
                used_bytes=used_bytes
            )
            
        except (AttributeError, ValueError) as e:
            logger.warning(f"Failed to get TensorFlow memory info: {e}")
            
            # Fallback to less accurate information
            return MemoryUsage(
                timestamp=time.time(),
                device=f"GPU:{device}",
                total_bytes=0,
                used_bytes=0
            )

class MemoryOptimizer:
    """Optimizes memory usage for ML workloads."""
    
    def __init__(self, config: MemoryConfig, framework: str = "pytorch"):
        self.config = config
        self.framework = framework.lower()
        self.tracker = MemoryTracker(framework)
        
        # Initialize memory optimizer based on framework
        if self.framework == "pytorch":
            self._init_pytorch()
        elif self.framework == "tensorflow":
            self._init_tensorflow()
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")
        
        # Initialize custom memory allocator if enabled
        if self.config.use_custom_allocator and RUST_MEMORY_AVAILABLE:
            self.memory_allocator = rust_memory.MemoryAllocator()
            logger.info("Initialized Rust custom memory allocator")
        else:
            self.memory_allocator = None
    
    def _init_pytorch(self) -> None:
        """Initialize PyTorch memory optimization settings."""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch not installed")
        
        import torch
        
        # Set cuDNN benchmark mode for optimal performance
        if self.config.cudnn_benchmark:
            torch.backends.cudnn.benchmark = True
        
        # Default to deterministic mode for debugging if not in benchmark mode
        if not self.config.cudnn_benchmark:
            torch.backends.cudnn.deterministic = True
        
        # Empty cache initially
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("Initialized PyTorch memory optimization settings")
    
    def _init_tensorflow(self) -> None:
        """Initialize TensorFlow memory optimization settings."""
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow not installed")
        
        import tensorflow as tf
        
        # Configure memory growth to avoid reserving all memory upfront
        if self.config.optimize_memory_usage:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                try:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    logger.info(f"Memory growth enabled for {len(gpus)} GPUs")
                except RuntimeError as e:
                    logger.error(f"Failed to set memory growth: {e}")
        
        logger.info("Initialized TensorFlow memory optimization settings")
    
    def optimize_for_training(self, model: Any, optimizer: Any = None) -> Tuple[Any, Any]:
        """
        Optimize model and optimizer for memory-efficient training.
        
        Args:
            model: The model to optimize
            optimizer: The optimizer to optimize (if applicable)
            
        Returns:
            Tuple of (optimized_model, optimized_optimizer)
        """
        if self.framework == "pytorch":
            return self._optimize_pytorch_training(model, optimizer)
        elif self.framework == "tensorflow":
            return self._optimize_tensorflow_training(model, optimizer)
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")
    
    def _optimize_pytorch_training(self, model: Any, optimizer: Any = None) -> Tuple[Any, Any]:
        """Optimize PyTorch model and optimizer for memory-efficient training."""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch not installed")
        
        import torch
        
        # Apply activation checkpointing if enabled
        if self.config.enable_activation_checkpointing:
            try:
                from torch.utils.checkpoint import checkpoint_sequential
                
                # Find potential modules for checkpointing
                # This is simplified - would need to be adapted for specific model architectures
                sequential_modules = []
                for name, module in model.named_children():
                    if isinstance(module, torch.nn.Sequential) and len(list(module.children())) > 2:
                        sequential_modules.append(module)
                
                # Apply checkpointing to sequential modules
                for i, module in enumerate(sequential_modules):
                    def create_checkpoint_wrapper(mod):
                        def custom_forward(*inputs):
                            return mod(*inputs)
                        return custom_forward
                    
                    if hasattr(model, name):
                        wrapped_module = lambda *inputs: checkpoint_sequential(
                            module, len(list(module.children())), create_checkpoint_wrapper(module)(*inputs)
                        )
                        setattr(model, name, wrapped_module)
                
                logger.info(f"Applied activation checkpointing to {len(sequential_modules)} modules")
            except Exception as e:
                logger.error(f"Failed to apply activation checkpointing: {e}")
        
        # Apply CPU offloading if enabled
        if self.config.enable_offloading and self.config.cpu_offload_params:
            try:
                # This is a simplified implementation - would use DeepSpeed or PyTorch FSDP
                # in a production environment
                device = next(model.parameters()).device
                cpu_parameters = {}
                
                # Move some parameters to CPU
                for name, param in model.named_parameters():
                    # Offload parameters from layers that aren't used frequently
                    if any(x in name.lower() for x in ['embedding', 'classifier']):
                        cpu_parameters[name] = param.data.clone()
                        param.data = param.data.to('cpu')
                        if self.config.cpu_offload_use_pin_memory:
                            cpu_parameters[name] = cpu_parameters[name].pin_memory()
                
                # Create hooks to move parameters back to GPU when needed
                def _pre_forward_hook(module, inputs):
                    # Move parameters back to GPU for computation
                    for name, param in module.named_parameters(recurse=False):
                        full_name = f"{module.__class__.__name__}.{name}"
                        if full_name in cpu_parameters:
                            param.data = cpu_parameters[full_name].to(device)
                
                def _post_forward_hook(module, inputs, outputs):
                    # Move parameters back to CPU after computation
                    for name, param in module.named_parameters(recurse=False):
                        full_name = f"{module.__class__.__name__}.{name}"
                        if full_name in cpu_parameters:
                            cpu_parameters[full_name] = param.data.clone()
                            if self.config.cpu_offload_use_pin_memory:
                                cpu_parameters[full_name] = cpu_parameters[full_name].pin_memory()
                            param.data = param.data.to('cpu')
                
                # Register hooks
                for name, module in model.named_modules():
                    if any(x in name.lower() for x in ['embedding', 'classifier']):
                        module.register_forward_pre_hook(_pre_forward_hook)
                        module.register_forward_hook(_post_forward_hook)
                
                logger.info("Applied CPU parameter offloading")
            except Exception as e:
                logger.error(f"Failed to apply CPU offloading: {e}")
        
        # Apply memory-efficient optimizer if enabled
        if self.config.memory_efficient_optimizer and optimizer is not None:
            try:
                # Check if optimizer supports memory efficiency
                if isinstance(optimizer, torch.optim.Adam):
                    # Replace with memory-optimized version if available
                    try:
                        from torch.optim import AdamW
                        
                        # Get current optimizer state
                        state_dict = optimizer.state_dict()
                        
                        # Create new optimizer with the same parameters
                        param_groups = optimizer.param_groups
                        new_optimizer = AdamW(
                            model.parameters(),
                            lr=param_groups[0]['lr'],
                            betas=param_groups[0].get('betas', (0.9, 0.999)),
                            eps=param_groups[0].get('eps', 1e-8),
                            weight_decay=param_groups[0].get('weight_decay', 0),
                            amsgrad=param_groups[0].get('amsgrad', False)
                        )
                        
                        # Load optimizer state
                        new_optimizer.load_state_dict(state_dict)
                        optimizer = new_optimizer
                        
                        logger.info("Replaced optimizer with memory-efficient AdamW")
                    except ImportError:
                        logger.warning("Memory-efficient AdamW not available")
            except Exception as e:
                logger.error(f"Failed to apply memory-efficient optimizer: {e}")
        
        return model, optimizer
    
    def _optimize_tensorflow_training(self, model: Any, optimizer: Any = None) -> Tuple[Any, Any]:
        """Optimize TensorFlow model and optimizer for memory-efficient training."""
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow not installed")
        
        import tensorflow as tf
        
        # Use mixed precision for memory efficiency
        if self.config.enable_mixed_precision and tf.config.list_physical_devices('GPU'):
            try:
                # Enable mixed precision
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                
                # If optimizer is provided, wrap with LossScaleOptimizer
                if optimizer is not None and not isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):
                    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
                
                logger.info("Enabled TensorFlow mixed precision training")
            except Exception as e:
                logger.error(f"Failed to enable mixed precision: {e}")
        
        # Apply gradient checkpointing if enabled
        if self.config.enable_activation_checkpointing and isinstance(model, tf.keras.Model):
            try:
                # Find layers that can benefit from gradient checkpointing
                checkpointed_layers = []
                for layer in model.layers:
                    if isinstance(layer, tf.keras.layers.Dense) or \
                       isinstance(layer, tf.keras.layers.Conv2D) or \
                       isinstance(layer, tf.keras.layers.LSTM):
                        checkpointed_layers.append(layer)
                
                if checkpointed_layers:
                    # Replace the forward pass with a checkpointed version
                    # This is a simplified example - would need adaptation for specific models
                    def checkpointed_call(layer, *args, **kwargs):
                        @tf.recompute_grad
                        def forward_pass(*forward_args, **forward_kwargs):
                            return layer.__call__(*forward_args, **forward_kwargs)
                        return forward_pass(*args, **kwargs)
                    
                    # Patch the call method
                    for layer in checkpointed_layers:
                        original_call = layer.__call__
                        layer.__call__ = lambda *args, **kwargs: checkpointed_call(layer, *args, **kwargs)
                    
                    logger.info(f"Applied gradient checkpointing to {len(checkpointed_layers)} layers")
            except Exception as e:
                logger.error(f"Failed to apply gradient checkpointing: {e}")
        
        return model, optimizer
    
    def optimize_for_inference(self, model: Any) -> Any:
        """
        Optimize model for memory-efficient inference.
        
        Args:
            model: The model to optimize
            
        Returns:
            Optimized model
        """
        if self.framework == "pytorch":
            return self._optimize_pytorch_inference(model)
        elif self.framework == "tensorflow":
            return self._optimize_tensorflow_inference(model)
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")
    
    def _optimize_pytorch_inference(self, model: Any) -> Any:
        """Optimize PyTorch model for memory-efficient inference."""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch not installed")
        
        import torch
        
        # Put model in evaluation mode
        model.eval()
        
        # Apply inference optimizations
        if self.config.release_inference_cache:
            torch.cuda.empty_cache()
        
        # Use custom allocator if available
        if self.memory_allocator is not None:
            try:
                # Hook into PyTorch's allocator
                # This is simplified - actual implementation would be more complex
                torch.cuda.memory.change_current_allocator(self.memory_allocator.get_allocator())
                logger.info("Applied custom memory allocator for inference")
            except Exception as e:
                logger.error(f"Failed to apply custom allocator: {e}")
        
        return model
    
    def _optimize_tensorflow_inference(self, model: Any) -> Any:
        """Optimize TensorFlow model for memory-efficient inference."""
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow not installed")
        
        import tensorflow as tf
        
        # Use mixed precision for memory efficiency
        if self.config.enable_mixed_precision and tf.config.list_physical_devices('GPU'):
            try:
                # Convert to mixed precision
                model = tf.keras.models.clone_model(model)
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                
                logger.info("Enabled TensorFlow mixed precision for inference")
            except Exception as e:
                logger.error(f"Failed to enable mixed precision: {e}")
        
        return model
    
    def analyze_memory_usage(self, memory_logs: List[MemoryUsage]) -> Dict[str, Any]:
        """
        Analyze memory usage statistics.
        
        Args:
            memory_logs: List of memory usage records
            
        Returns:
            Dictionary with analysis results
        """
        if not memory_logs:
            return {"error": "No memory usage data provided"}
        
        # Extract basic statistics
        timestamps = [log.timestamp for log in memory_logs]
        used_bytes = [log.used_bytes for log in memory_logs]
        total_bytes = [log.total_bytes for log in memory_logs if log.total_bytes > 0]
        utilization = [log.utilization for log in memory_logs if log.total_bytes > 0]
        
        # Calculate summary statistics
        stats = {
            "duration_seconds": max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0,
            "num_samples": len(memory_logs),
            "max_memory_used_gb": max(used_bytes) / (1024 ** 3) if used_bytes else 0,
            "min_memory_used_gb": min(used_bytes) / (1024 ** 3) if used_bytes else 0,
            "avg_memory_used_gb": np.mean(used_bytes) / (1024 ** 3) if used_bytes else 0,
            "max_utilization_percent": max(utilization) if utilization else 0,
            "avg_utilization_percent": np.mean(utilization) if utilization else 0,
            "total_memory_gb": max(total_bytes) / (1024 ** 3) if total_bytes else 0,
        }
        
        # Calculate memory growth over time
        if len(timestamps) > 1:
            # Calculate growth rate (bytes per second)
            time_diff = max(timestamps) - min(timestamps)
            memory_diff = used_bytes[-1] - used_bytes[0]
            
            if time_diff > 0:
                growth_rate = memory_diff / time_diff
                stats["memory_growth_mb_per_second"] = growth_rate / (1024 ** 2)
                stats["memory_growth_pattern"] = "increasing" if growth_rate > 0 else "decreasing" if growth_rate < 0 else "stable"
            
            # Detect memory spikes
            memory_gb = np.array(used_bytes) / (1024 ** 3)
            rolling_avg = np.convolve(memory_gb, np.ones(5)/5, mode='valid')
            if len(rolling_avg) > 1:
                spikes = []
                for i in range(len(rolling_avg) - 1):
                    diff = rolling_avg[i+1] - rolling_avg[i]
                    if diff > 0.1:  # Spike defined as 100MB sudden increase
                        spikes.append({
                            "time_index": i + 2,  # Adjust for convolution offset
                            "timestamp": timestamps[i + 2] if i + 2 < len(timestamps) else 0,
                            "magnitude_gb": diff
                        })
                
                stats["memory_spikes"] = spikes
                stats["num_spikes"] = len(spikes)
        
        # Analyze fragmentation if available
        if any(log.fragmentation > 0 for log in memory_logs):
            fragmentation = [log.fragmentation for log in memory_logs if log.fragmentation > 0]
            stats["max_fragmentation"] = max(fragmentation)
            stats["avg_fragmentation"] = np.mean(fragmentation)
            stats["fragmentation_severity"] = "high" if stats["max_fragmentation"] > 0.3 else "moderate" if stats["max_fragmentation"] > 0.1 else "low"
        
        return stats
    
    def suggest_optimizations(self, analysis: Dict[str, Any]) -> List[str]:
        """
        Suggest memory optimizations based on analysis.
        
        Args:
            analysis: Memory usage analysis results
            
        Returns:
            List of optimization suggestions
        """
        suggestions = []
        
        # Check if analysis has expected fields
        if "error" in analysis:
            return ["Unable to provide suggestions due to analysis error"]
        
        # Check for high memory utilization
        if analysis.get("max_utilization_percent", 0) > 90:
            suggestions.append("High memory utilization (>90%) detected. Consider reducing batch size or enabling memory optimizations like gradient checkpointing.")
        
        # Check for memory growth pattern
        if analysis.get("memory_growth_pattern") == "increasing":
            suggestions.append("Continuous memory growth detected, which may indicate a memory leak. Check for tensor accumulation or uncollected references.")
        
        # Check for memory spikes
        if analysis.get("num_spikes", 0) > 0:
            suggestions.append(f"Detected {analysis['num_spikes']} memory spikes. Examine operations at these points to optimize peak memory usage.")
        
        # Check for fragmentation
        if analysis.get("fragmentation_severity") == "high":
            suggestions.append("High memory fragmentation detected. Consider periodically clearing cache or using a custom memory allocator.")
        
        # Suggest specific optimizations based on framework
        if self.framework == "pytorch":
            if not self.config.enable_activation_checkpointing and analysis.get("max_utilization_percent", 0) > 80:
                suggestions.append("Enable activation checkpointing to reduce memory usage at the cost of some recomputation.")
            
            if not self.config.memory_efficient_optimizer:
                suggestions.append("Use memory-efficient optimizers like Adam with fused operations or AdamW.")
            
            if analysis.get("max_memory_used_gb", 0) > 16:
                suggestions.append("Consider using mixed precision training (torch.cuda.amp) to reduce memory usage by almost 50%.")
                
            if not self.config.enable_offloading and analysis.get("max_utilization_percent", 0) > 95:
                suggestions.append("For very large models, enable parameter offloading to CPU when not in use.")
                
        elif self.framework == "tensorflow":
            if not self.config.enable_mixed_precision:
                suggestions.append("Enable TensorFlow mixed precision policy to reduce memory usage.")
            
            if analysis.get("max_utilization_percent", 0) > 80:
                suggestions.append("Use tf.recompute_grad for memory-intensive layers to trade computation for memory.")
            
            if not self.config.optimize_memory_usage:
                suggestions.append("Enable memory growth for TensorFlow to avoid allocating all GPU memory upfront.")
        
        # General suggestions
        if len(suggestions) == 0:
            suggestions.append("Memory usage appears efficient. No specific optimizations needed.")
        
        return suggestions

# ... rest of the code ...

### 4. Rust - Performance Profiling

```rust
// rust/src/profiling/trace.rs
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, BufWriter, Read, Write};
use std::path::Path;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use serde::{Deserialize, Serialize};
use thiserror::Error;

use crate::utils::logging::get_logger;

/// Errors that can occur during tracing
#[derive(Error, Debug)]
pub enum TraceError {
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("JSON error: {0}")]
    Json(#[from] serde_json::Error),
    
    #[error("Invalid state: {0}")]
    InvalidState(String),
}

/// Type alias for trace operation result
pub type TraceResult<T> = Result<T, TraceError>;

/// A trace event in Chrome Trace Event format
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceEvent {
    /// Event name
    pub name: String,
    
    /// Event phase (B/E for begin/end, X for complete, etc.)
    pub ph: String,
    
    /// Timestamp in microseconds
    pub ts: u64,
    
    /// Duration in microseconds (only for complete events)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub dur: Option<u64>,
    
    /// Process ID
    pub pid: u32,
    
    /// Thread ID
    pub tid: u32,
    
    /// Event category
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cat: Option<String>,
    
    /// Event arguments
    #[serde(default)]
    pub args: HashMap<String, serde_json::Value>,
}

/// A trace in Chrome Trace Event format
#[derive(Debug, Serialize, Deserialize)]
pub struct Trace {
    /// List of trace events
    pub traceEvents: Vec<TraceEvent>,
    
    /// Display time unit (ms, us, etc.)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub displayTimeUnit: Option<String>,
    
    /// System details
    #[serde(skip_serializing_if = "Option::is_none")]
    pub systemTraceEvents: Option<serde_json::Value>,
    
    /// Metadata
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

impl Trace {
    /// Create a new empty trace
    pub fn new() -> Self {
        Self {
            traceEvents: Vec::new(),
            displayTimeUnit: Some("ms".to_string()),
            systemTraceEvents: None,
            metadata: None,
        }
    }
    
    /// Add a trace event
    pub fn add_event(&mut self, event: TraceEvent) {
        self.traceEvents.push(event);
    }
    
    /// Add multiple trace events
    pub fn add_events(&mut self, events: Vec<TraceEvent>) {
        self.traceEvents.extend(events);
    }
    
    /// Save trace to a file in Chrome Trace Event format
    pub fn save_to_file<P: AsRef<Path>>(&self, path: P) -> TraceResult<()> {
        let file = File::create(path)?;
        let writer = BufWriter::new(file);
        serde_json::to_writer_pretty(writer, self)?;
        Ok(())
    }
    
    /// Load trace from a file in Chrome Trace Event format
    pub fn load_from_file<P: AsRef<Path>>(path: P) -> TraceResult<Self> {
        let file = File::open(path)?;
        let reader = BufReader::new(file);
        let trace = serde_json::from_reader(reader)?;
        Ok(trace)
    }
    
    /// Combine with another trace
    pub fn combine(&mut self, other: &Self) {
        self.traceEvents.extend(other.traceEvents.clone());
    }
    
    /// Get events filtered by name
    pub fn get_events_by_name(&self, name: &str) -> Vec<&TraceEvent> {
        self.traceEvents.iter().filter(|e| e.name == name).collect()
    }
    
    /// Get events filtered by category
    pub fn get_events_by_category(&self, category: &str) -> Vec<&TraceEvent> {
        self.traceEvents
            .iter()
            .filter(|e| e.cat.as_ref().map_or(false, |c| c == category))
            .collect()
    }
    
    /// Calculate total duration across all events
    pub fn calculate_total_duration(&self) -> u64 {
        let start = self.traceEvents.iter().map(|e| e.ts).min().unwrap_or(0);
        let end = self
            .traceEvents
            .iter()
            .map(|e| e.ts + e.dur.unwrap_or(0))
            .max()
            .unwrap_or(0);
        end - start
    }
}

/// A trace activity to be timed
pub struct TraceActivity {
    /// Name of the activity
    name: String,
    
    /// Category of the activity
    category: Option<String>,
    
    /// When the activity started
    start_time: Instant,
    
    /// Process ID
    pid: u32,
    
    /// Thread ID
    tid: u32,
    
    /// Additional arguments
    args: HashMap<String, serde_json::Value>,
}

impl TraceActivity {
    /// Create a new trace activity
    pub fn new(
        name: &str,
        category: Option<&str>,
        pid: u32,
        tid: u32,
    ) -> Self {
        Self {
            name: name.to_string(),
            category: category.map(|s| s.to_string()),
            start_time: Instant::now(),
            pid,
            tid,
            args: HashMap::new(),
        }
    }
    
    /// Add an argument to the activity
    pub fn with_arg<T: Serialize>(mut self, key: &str, value: T) -> TraceResult<Self> {
        let json_value = serde_json::to_value(value)?;
        self.args.insert(key.to_string(), json_value);
        Ok(self)
    }
    
    /// End the activity and get the trace event
    pub fn end(self) -> TraceEvent {
        let duration = self.start_time.elapsed();
        let duration_us = duration.as_micros() as u64;
        
        // Get current time in microseconds since epoch
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or(Duration::from_secs(0))
            .as_micros() as u64;
        
        let start_ts = now - duration_us;
        
        TraceEvent {
            name: self.name,
            ph: "X".to_string(),  // Complete event
            ts: start_ts,
            dur: Some(duration_us),
            pid: self.pid,
            tid: self.tid,
            cat: self.category,
            args: self.args,
        }
    }
}

/// A tracer for performance profiling
pub struct Tracer {
    /// Trace data
    trace: Arc<Mutex<Trace>>,
    
    /// Logger
    logger: slog::Logger,
}

impl Tracer {
    /// Create a new tracer
    pub fn new() -> Self {
        Self {
            trace: Arc::new(Mutex::new(Trace::new())),
            logger: get_logger(),
        }
    }
    
    /// Start a trace activity
    pub fn start_activity(
        &self,
        name: &str,
        category: Option<&str>,
    ) -> TraceActivity {
        let pid = std::process::id();
        let tid = unsafe { libc::pthread_self() } as u32;
        
        TraceActivity::new(name, category, pid, tid)
    }
    
    /// Record a completed trace activity
    pub fn record_activity(&self, activity: TraceActivity) {
        let event = activity.end();
        
        if let Ok(mut trace) = self.trace.lock() {
            trace.add_event(event);
        } else {
            slog::warn!(self.logger, "Failed to lock trace for recording activity");
        }
    }
    
    /// Add a custom event
    pub fn add_event(&self, event: TraceEvent) {
        if let Ok(mut trace) = self.trace.lock() {
            trace.add_event(event);
        } else {
            slog::warn!(self.logger, "Failed to lock trace for adding event");
        }
    }
    
    /// Save trace to a file
    pub fn save_to_file<P: AsRef<Path>>(&self, path: P) -> TraceResult<()> {
        if let Ok(trace) = self.trace.lock() {
            trace.save_to_file(path)?;
        } else {
            return Err(TraceError::InvalidState(
                "Failed to lock trace for saving".to_string(),
            ));
        }
        Ok(())
    }
    
    /// Get a clone of the current trace
    pub fn get_trace(&self) -> TraceResult<Trace> {
        if let Ok(trace) = self.trace.lock() {
            Ok(trace.clone())
        } else {
            Err(TraceError::InvalidState(
                "Failed to lock trace for cloning".to_string(),
            ))
        }
    }
    
    /// Clear the trace
    pub fn clear(&self) {
        if let Ok(mut trace) = self.trace.lock() {
            trace.traceEvents.clear();
        } else {
            slog::warn!(self.logger, "Failed to lock trace for clearing");
        }
    }
}

/// A trace scope that automatically records an activity when dropped
pub struct TraceScope<'a> {
    tracer: &'a Tracer,
    activity: Option<TraceActivity>,
}

impl<'a> TraceScope<'a> {
    /// Create a new trace scope
    pub fn new(
        tracer: &'a Tracer,
        name: &str,
        category: Option<&str>,
    ) -> Self {
        let activity = tracer.start_activity(name, category);
        Self {
            tracer,
            activity: Some(activity),
        }
    }
    
    /// Add an argument to the activity
    pub fn with_arg<T: Serialize>(&mut self, key: &str, value: T) -> TraceResult<&mut Self> {
        if let Some(activity) = self.activity.take() {
            self.activity = Some(activity.with_arg(key, value)?);
        }
        Ok(self)
    }
}

impl<'a> Drop for TraceScope<'a> {
    fn drop(&mut self) {
        if let Some(activity) = self.activity.take() {
            self.tracer.record_activity(activity);
        }
    }
}

/// Trace analysis methods
pub mod analysis {
    use super::*;
    use std::collections::{HashMap, HashSet};
    use rayon::prelude::*;
    
    /// Metrics for a trace event
    pub struct EventMetrics {
        /// Event name
        pub name: String,
        
        /// Event category
        pub category: Option<String>,
        
        /// Count of events
        pub count: usize,
        
        /// Total duration in microseconds
        pub total_duration_us: u64,
        
        /// Average duration in microseconds
        pub avg_duration_us: f64,
        
        /// Minimum duration in microseconds
        pub min_duration_us: u64,
        
        /// Maximum duration in microseconds
        pub max_duration_us: u64,
        
        /// Standard deviation of duration
        pub stddev_duration_us: f64,
        
        /// List of process IDs that contain this event
        pub pids: HashSet<u32>,
        
        /// List of thread IDs that contain this event
        pub tids: HashSet<u32>,
    }
    
    /// Calculate metrics for all events in a trace
    pub fn calculate_metrics(trace: &Trace) -> HashMap<String, EventMetrics> {
        let mut metrics = HashMap::new();
        
        // Group events by name
        let mut events_by_name: HashMap<String, Vec<&TraceEvent>> = HashMap::new();
        
        for event in &trace.traceEvents {
            if event.ph == "X" && event.dur.is_some() {
                events_by_name
                    .entry(event.name.clone())
                    .or_default()
                    .push(event);
            }
        }
        
        // Calculate metrics for each event name
        for (name, events) in events_by_name {
            let count = events.len();
            
            let durations: Vec<u64> = events
                .iter()
                .filter_map(|e| e.dur)
                .collect();
            
            if durations.is_empty() {
                continue;
            }
            
            let total_duration_us = durations.iter().sum();
            let avg_duration_us = total_duration_us as f64 / count as f64;
            let min_duration_us = *durations.iter().min().unwrap_or(&0);
            let max_duration_us = *durations.iter().max().unwrap_or(&0);
            
            // Calculate standard deviation
            let variance = durations
                .iter()
                .map(|&d| {
                    let diff = d as f64 - avg_duration_us;
                    diff * diff
                })
                .sum::<f64>()
                / count as f64;
            
            let stddev_duration_us = variance.sqrt();
            
            // Collect process and thread IDs
            let pids: HashSet<u32> = events.iter().map(|e| e.pid).collect();
            let tids: HashSet<u32> = events.iter().map(|e| e.tid).collect();
            
            // Get category (assume all events of the same name have the same category)
            let category = events.first().and_then(|e| e.cat.clone());
            
            metrics.insert(
                name.clone(),
                EventMetrics {
                    name,
                    category,
                    count,
                    total_duration_us,
                    avg_duration_us,
                    min_duration_us,
                    max_duration_us,
                    stddev_duration_us,
                    pids,
                    tids,
                },
            );
        }
        
        metrics
    }
    
    /// Identify hotspots in a trace
    pub fn identify_hotspots(trace: &Trace, threshold_percent: f64) -> Vec<(String, f64)> {
        let metrics = calculate_metrics(trace);
        let total_duration = trace.calculate_total_duration();
        
        if total_duration == 0 {
            return Vec::new();
        }
        
        // Calculate percentage of total time for each event
        let mut percentages: Vec<(String, f64)> = metrics
            .iter()
            .map(|(name, m)| {
                let percent = (m.total_duration_us as f64 / total_duration as f64) * 100.0;
                (name.clone(), percent)
            })
            .collect();
        
        // Sort by percentage (descending)
        percentages.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        
        // Filter by threshold
        percentages
            .into_iter()
            .filter(|(_, percent)| *percent >= threshold_percent)
            .collect()
    }
    
    /// Identify thread imbalance in a trace
    pub fn identify_thread_imbalance(trace: &Trace) -> Option<(f64, HashMap<u32, u64>)> {
        // Calculate total time spent by each thread
        let mut thread_times: HashMap<u32, u64> = HashMap::new();
        
        for event in &trace.traceEvents {
            if event.ph == "X" && event.dur.is_some() {
                let duration = event.dur.unwrap_or(0);
                *thread_times.entry(event.tid).or_default() += duration;
            }
        }
        
        if thread_times.len() <= 1 {
            return None;
        }
        
        // Find max and min thread times
        let max_time = thread_times.values().max().unwrap_or(&0);
        let min_time = thread_times.values().min().unwrap_or(&0);
        
        if *min_time == 0 {
            return None;
        }
        
        // Calculate imbalance ratio
        let imbalance_ratio = *max_time as f64 / *min_time as f64;
        
        Some((imbalance_ratio, thread_times))
    }
    
    /// Identify communication patterns in a trace
    pub fn identify_communication_patterns(trace: &Trace) -> HashMap<String, usize> {
        let comm_keywords = [
            "nccl", "mpi", "allreduce", "allgather", "send", "recv", "barrier", "collective",
        ];
        
        let mut patterns = HashMap::new();
        
        for event in &trace.traceEvents {
            // Check if event name or category contains communication keywords
            let name_matches = comm_keywords
                .iter()
                .any(|&kw| event.name.to_lowercase().contains(kw));
            
            let cat_matches = event
                .cat
                .as_ref()
                .map_or(false, |c| comm_keywords.iter().any(|&kw| c.to_lowercase().contains(kw)));
            
            if name_matches || cat_matches {
                let key = if name_matches {
                    event.name.clone()
                } else {
                    event.cat.clone().unwrap_or_else(|| "unknown".to_string())
                };
                
                *patterns.entry(key).or_default() += 1;
            }
        }
        
        patterns
    }
    
    /// Calculate critical path in a trace
    pub fn calculate_critical_path(trace: &Trace) -> Vec<&TraceEvent> {
        // This is a simplified critical path algorithm that doesn't account for dependencies
        // In a real implementation, you would need to model the DAG of dependencies
        
        // Sort events by start time
        let mut sorted_events: Vec<&TraceEvent> = trace
            .traceEvents
            .iter()
            .filter(|e| e.ph == "X" && e.dur.is_some())
            .collect();
        
        sorted_events.sort_by_key(|e| e.ts);
        
        // Simple greedy algorithm
        let mut critical_path = Vec::new();
        let mut current_end = 0;
        
        for event in sorted_events {
            if event.ts >= current_end {
                critical_path.push(event);
                current_end = event.ts + event.dur.unwrap_or(0);
            } else if event.ts + event.dur.unwrap_or(0) > current_end {
                critical_path.push(event);
                current_end = event.ts + event.dur.unwrap_or(0);
            }
        }
        
        critical_path
    }
    
    /// Analyze parallelism in a trace
    pub fn analyze_parallelism(trace: &Trace) -> HashMap<String, f64> {
        let mut results = HashMap::new();
        
        // Calculate time ranges for each event
        let mut time_ranges: Vec<(u64, u64)> = trace
            .traceEvents
            .iter()
            .filter(|e| e.ph == "X" && e.dur.is_some())
            .map(|e| (e.ts, e.ts + e.dur.unwrap_or(0)))
            .collect();
        
        if time_ranges.is_empty() {
            return results;
        }
        
        // Sort by start time
        time_ranges.sort_by_key(|&(start, _)| start);
        
        // Calculate overall time range
        let overall_start = time_ranges.first().map(|&(start, _)| start).unwrap_or(0);
        let overall_end = time_ranges
            .iter()
            .map(|&(_, end)| end)
            .max()
            .unwrap_or(0);
        let overall_duration = overall_end - overall_start;
        
        if overall_duration == 0 {
            return results;
        }
        
        // Calculate total work time
        let total_work_time: u64 = time_ranges.iter().map(|&(_, end)| end).sum::<u64>()
            - time_ranges.iter().map(|&(start, _)| start).sum::<u64>();
        
        // Calculate parallelism
        let avg_parallelism = total_work_time as f64 / overall_duration as f64;
        results.insert("average_parallelism".to_string(), avg_parallelism);
        
        // Calculate peak parallelism using a simple algorithm
        let mut timeline = Vec::new();
        for (start, end) in time_ranges {
            timeline.push((start, 1));  // Event start adds 1 to parallelism
            timeline.push((end, -1));   // Event end subtracts 1 from parallelism
        }
        
        timeline.sort_by_key(|&(time, _)| time);
        
        let mut current_parallelism = 0;
        let mut max_parallelism = 0;
        
        for (_, delta) in timeline {
            current_parallelism += delta;
            max_parallelism = max_parallelism.max(current_parallelism);
        }
        
        results.insert("peak_parallelism".to_string(), max_parallelism as f64);
        
        // Calculate parallelism efficiency
        if max_parallelism > 0 {
            let parallelism_efficiency = avg_parallelism / max_parallelism as f64;
            results.insert("parallelism_efficiency".to_string(), parallelism_efficiency);
        }
        
        results
    }
}

impl AllocStats {
    /// Create a new allocation statistics object
    pub fn new() -> Self {
        Self {
            last_reset: Some(chrono::Utc::now()),
            ..Default::default()
        }
    }
    
    /// Reset statistics
    pub fn reset(&mut self) {
        *self = AllocStats::new();
    }
    
    /// Record an allocation
    pub fn record_allocation(&mut self, size: usize) {
        self.total_allocations += 1;
        self.current_allocations += 1;
        self.total_bytes_allocated += size;
        self.current_bytes_allocated += size;
        
        if self.current_allocations > self.peak_allocations {
            self.peak_allocations = self.current_allocations;
        }
        
        if self.current_bytes_allocated > self.peak_bytes_allocated {
            self.peak_bytes_allocated = self.current_bytes_allocated;
        }
        
        if size > self.largest_allocation {
            self.largest_allocation = size;
        }
        
        if self.smallest_allocation == 0 || size < self.smallest_allocation {
            self.smallest_allocation = size;
        }
        
        // Update size distribution (bucket by power of 2)
        let bucket_size = size.next_power_of_two();
        *self.size_distribution.entry(bucket_size).or_insert(0) += 1;
        
        // Update average allocation size
        self.average_allocation_size = self.total_bytes_allocated as f64 / self.total_allocations as f64;
    }
    
    /// Record a deallocation
    pub fn record_deallocation(&mut self, size: usize) {
        self.total_deallocations += 1;
        self.current_allocations -= 1;
        self.total_bytes_deallocated += size;
        self.current_bytes_allocated -= size;
    }
    
    /// Record a cache hit
    pub fn record_cache_hit(&mut self) {
        self.cache_hits += 1;
    }
    
    /// Record a cache miss
    pub fn record_cache_miss(&mut self) {
        self.cache_misses += 1;
    }
    
    /// Record an allocation failure
    pub fn record_allocation_failure(&mut self) {
        self.allocation_failures += 1;
    }
    
    /// Record a reallocation
    pub fn record_reallocation(&mut self) {
        self.reallocations += 1;
    }
    
    /// Get cache hit rate
    pub fn cache_hit_rate(&self) -> f64 {
        let total = self.cache_hits + self.cache_misses;
        if total == 0 {
            0.0
        } else {
            self.cache_hits as f64 / total as f64
        }
    }
    
    /// Get allocation success rate
    pub fn allocation_success_rate(&self) -> f64 {
        let total = self.total_allocations + self.allocation_failures;
        if total == 0 {
            1.0
        } else {
            self.total_allocations as f64 / total as f64
        }
    }
    
    /// Get memory utilization
    pub fn memory_utilization(&self) -> f64 {
        if self.peak_bytes_allocated == 0 {
            0.0
        } else {
            self.current_bytes_allocated as f64 / self.peak_bytes_allocated as f64
        }
    }
    
    /// Get fragmentation ratio
    pub fn fragmentation_ratio(&self) -> f64 {
        // This is a simplified approximation - real fragmentation 
        // would need to analyze actual memory layout
        if self.current_bytes_allocated == 0 {
            0.0
        } else {
            // Ratio of deallocations to allocations as a proxy for fragmentation
            self.total_deallocations as f64 / self.total_allocations as f64
        }
    }
}

/// A memory allocation
#[derive(Debug)]
struct Allocation {
    /// Pointer to the allocated memory
    ptr: NonNull<u8>,
    
    /// Size of the allocation
    size: usize,
    
    /// Layout of the allocation
    layout: Layout,
    
    /// Timestamp of the allocation
    timestamp: Instant,
}

/// A memory allocator that tracks allocations
pub struct TrackingAllocator<A: GlobalAlloc> {
    /// The underlying allocator
    inner: A,
    
    /// Statistics about allocations
    stats: Arc<RwLock<AllocStats>>,
    
    /// Active allocations
    active_allocations: Arc<DashMap<usize, Allocation>>,
    
    /// Allocation timestamp tracking
    allocation_times: Arc<DashMap<usize, Instant>>,
    
    /// Logger
    logger: slog::Logger,
}

impl<A: GlobalAlloc> TrackingAllocator<A> {
    /// Create a new tracking allocator
    pub fn new(inner: A) -> Self {
        Self {
            inner,
            stats: Arc::new(RwLock::new(AllocStats::new())),
            active_allocations: Arc::new(DashMap::new()),
            allocation_times: Arc::new(DashMap::new()),
            logger: get_logger(),
        }
    }
    
    /// Get allocation statistics
    pub fn get_stats(&self) -> AllocStats {
        match self.stats.read() {
            Ok(stats) => stats.clone(),
            Err(_) => {
                slog::warn!(self.logger, "Failed to read allocation stats");
                AllocStats::new()
            }
        }
    }
    
    /// Reset allocation statistics
    pub fn reset_stats(&self) {
        if let Ok(mut stats) = self.stats.write() {
            stats.reset();
        } else {
            slog::warn!(self.logger, "Failed to reset allocation stats");
        }
    }
    
    /// Get active allocations
    pub fn get_active_allocations(&self) -> Vec<(usize, usize, Duration)> {
        self.active_allocations
            .iter()
            .map(|entry| {
                let alloc = entry.value();
                let ptr = alloc.ptr.as_ptr() as usize;
                let size = alloc.size;
                let age = alloc.timestamp.elapsed();
                (ptr, size, age)
            })
            .collect()
    }
    
    /// Find the allocation corresponding to a pointer
    fn find_allocation(&self, ptr: *mut u8) -> Option<Allocation> {
        let ptr_val = ptr as usize;
        self.active_allocations.remove(&ptr_val).map(|(_, alloc)| alloc)
    }
    
    /// Record long-lived allocations
    pub fn identify_long_lived_allocations(&self, threshold: Duration) -> Vec<(usize, usize, Duration)> {
        let now = Instant::now();
        
        self.active_allocations
            .iter()
            .filter_map(|entry| {
                let alloc = entry.value();
                let age = now.duration_since(alloc.timestamp);
                
                if age >= threshold {
                    Some((alloc.ptr.as_ptr() as usize, alloc.size, age))
                } else {
                    None
                }
            })
            .collect()
    }
    
    /// Find the largest allocations
    pub fn find_largest_allocations(&self, limit: usize) -> Vec<(usize, usize, Duration)> {
        let mut allocations: Vec<(usize, usize, Duration)> = self.active_allocations
            .iter()
            .map(|entry| {
                let alloc = entry.value();
                (
                    alloc.ptr.as_ptr() as usize,
                    alloc.size,
                    alloc.timestamp.elapsed(),
                )
            })
            .collect();
        
        allocations.sort_unstable_by(|a, b| b.1.cmp(&a.1));
        allocations.truncate(limit);
        
        allocations
    }
}

unsafe impl<A: GlobalAlloc> GlobalAlloc for TrackingAllocator<A> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let ptr = self.inner.alloc(layout);
        
        if !ptr.is_null() {
            let ptr_val = ptr as usize;
            let size = layout.size();
            
            // Record the allocation in stats
            if let Ok(mut stats) = self.stats.write() {
                stats.record_allocation(size);
            }
            
            // Record the allocation in active allocations
            let allocation = Allocation {
                ptr: NonNull::new_unchecked(ptr),
                size,
                layout,
                timestamp: Instant::now(),
            };
            
            self.active_allocations.insert(ptr_val, allocation);
        } else {
            // Record allocation failure
            if let Ok(mut stats) = self.stats.write() {
                stats.record_allocation_failure();
            }
        }
        
        ptr
    }
    
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        if let Some(allocation) = self.find_allocation(ptr) {
            // Record the deallocation in stats
            if let Ok(mut stats) = self.stats.write() {
                stats.record_deallocation(allocation.size);
            }
        }
        
        self.inner.dealloc(ptr, layout);
    }
    
    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
        let old_allocation = self.find_allocation(ptr);
        
        let new_ptr = self.inner.realloc(ptr, layout, new_size);
        
        if !new_ptr.is_null() {
            let new_ptr_val = new_ptr as usize;
            
            // Record statistics
            if let Ok(mut stats) = self.stats.write() {
                if let Some(old_alloc) = &old_allocation {
                    stats.record_deallocation(old_alloc.size);
                }
                stats.record_allocation(new_size);
                stats.record_reallocation();
            }
            
            // Record the new allocation in active allocations
            let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());
            let allocation = Allocation {
                ptr: NonNull::new_unchecked(new_ptr),
                size: new_size,
                layout: new_layout,
                timestamp: old_allocation
                    .map_or_else(Instant::now, |alloc| alloc.timestamp),
            };
            
            self.active_allocations.insert(new_ptr_val, allocation);
        } else {
            // Record allocation failure
            if let Ok(mut stats) = self.stats.write() {
                stats.record_allocation_failure();
            }
        }
        
        new_ptr
    }
}

/// A memory pool for reusing allocations
pub struct MemoryPool {
    /// Statistics for this pool
    stats: Arc<RwLock<AllocStats>>,
    
    /// Pools of free blocks organized by size class
    free_pools: DashMap<usize, Vec<NonNull<u8>>>,
    
    /// Size class for each allocation
    allocation_sizes: DashMap<usize, usize>,
    
    /// Size classes for the pool
    size_classes: Vec<usize>,
    
    /// Underlying system allocator
    system_alloc: System,
    
    /// Logger
    logger: slog::Logger,
}

impl MemoryPool {
    /// Create a new memory pool
    pub fn new() -> Self {
        // Define size classes (powers of 2 for simplicity)
        let size_classes = (0..32)
            .map(|i| 1usize << i)
            .collect();
        
        Self {
            stats: Arc::new(RwLock::new(AllocStats::new())),
            free_pools: DashMap::new(),
            allocation_sizes: DashMap::new(),
            size_classes,
            system_alloc: System,
            logger: get_logger(),
        }
    }
    
    /// Get the appropriate size class for a requested size
    fn get_size_class(&self, size: usize) -> usize {
        self.size_classes
            .iter()
            .find(|&&sc| sc >= size)
            .copied()
            .unwrap_or_else(|| size.next_power_of_two())
    }
    
    /// Allocate memory
    pub fn allocate(&self, layout: Layout) -> *mut u8 {
        let size = layout.size().max(layout.align());
        let size_class = self.get_size_class(size);
        
        // Try to reuse a block from the free pool
        if let Some(mut pool) = self.free_pools.get_mut(&size_class) {
            if let Some(ptr) = pool.pop() {
                let ptr_val = ptr.as_ptr() as usize;
                self.allocation_sizes.insert(ptr_val, size_class);
                
                // Record statistics
                if let Ok(mut stats) = self.stats.write() {
                    stats.record_allocation(size_class);
                    stats.record_cache_hit();
                }
                
                return ptr.as_ptr();
            }
        }
        
        // No block available, allocate a new one
        let padding = size_class - size;
        let padded_layout = if padding > 0 {
            let align = layout.align().max(8);
            Layout::from_size_align(size_class, align).unwrap_or(layout)
        } else {
            layout
        };
        
        let ptr = unsafe { self.system_alloc.alloc(padded_layout) };
        
        if !ptr.is_null() {
            let ptr_val = ptr as usize;
            self.allocation_sizes.insert(ptr_val, size_class);
            
            // Record statistics
            if let Ok(mut stats) = self.stats.write() {
                stats.record_allocation(size_class);
                stats.record_cache_miss();
            }
        } else {
            // Record allocation failure
            if let Ok(mut stats) = self.stats.write() {
                stats.record_allocation_failure();
            }
        }
        
        ptr
    }
    
    /// Deallocate memory
    pub fn deallocate(&self, ptr: *mut u8, layout: Layout) {
        if ptr.is_null() {
            return;
        }
        
        let ptr_val = ptr as usize;
        
        // Get the size class for this allocation
        if let Some((_, size_class)) = self.allocation_sizes.remove(&ptr_val) {
            // Record deallocation in stats
            if let Ok(mut stats) = self.stats.write() {
                stats.record_deallocation(size_class);
            }
            
            // Return the block to the free pool if it's not too large
            if size_class <= 1024 * 1024 {  // Don't cache blocks larger than 1MB
                let ptr_nn = unsafe { NonNull::new_unchecked(ptr) };
                
                self.free_pools
                    .entry(size_class)
                    .or_insert_with(Vec::new)
                    .push(ptr_nn);
            } else {
                // For large blocks, just return to the system
                unsafe {
                    self.system_alloc.dealloc(ptr, layout);
                }
            }
        } else {
            // Unknown allocation, just return to the system
            unsafe {
                self.system_alloc.dealloc(ptr, layout);
            }
        }
    }
    
    /// Get allocation statistics
    pub fn get_stats(&self) -> AllocStats {
        match self.stats.read() {
            Ok(stats) => stats.clone(),
            Err(_) => {
                slog::warn!(self.logger, "Failed to read allocation stats");
                AllocStats::new()
            }
        }
    }
    
    /// Trim the memory pool, returning unused memory to the system
    pub fn trim(&self, threshold: usize) -> usize {
        let mut blocks_freed = 0;
        
        for mut entry in self.free_pools.iter_mut() {
            let size_class = *entry.key();
            let pool = entry.value_mut();
            
            // If this pool has more than threshold blocks, return excess to the system
            if pool.len() > threshold {
                let excess = pool.len() - threshold;
                blocks_freed += excess;
                
                // Create a fake layout for deallocation
                let layout = Layout::from_size_align(size_class, 8).unwrap();
                
                for _ in 0..excess {
                    if let Some(ptr) = pool.pop() {
                        unsafe {
                            self.system_alloc.dealloc(ptr.as_ptr(), layout);
                        }
                    }
                }
            }
        }
        
        blocks_freed
    }
    
    /// Release all cached memory
    pub fn release_all(&self) -> usize {
        let mut blocks_freed = 0;
        
        for mut entry in self.free_pools.iter_mut() {
            let size_class = *entry.key();
            let pool = entry.value_mut();
            
            blocks_freed += pool.len();
            
            // Create a fake layout for deallocation
            let layout = Layout::from_size_align(size_class, 8).unwrap();
            
            for ptr in pool.drain(..) {
                unsafe {
                    self.system_alloc.dealloc(ptr.as_ptr(), layout);
                }
            }
        }
        
        blocks_freed
    }
}

unsafe impl GlobalAlloc for MemoryPool {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        self.allocate(layout)
    }
    
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.deallocate(ptr, layout)
    }
}

/// Thread-local allocator for per-thread memory pools
pub struct ThreadLocalAllocator {
    /// The memory pool for this thread
    pool: RefCell<MemoryPool>,
    
    /// Logger
    logger: slog::Logger,
}

impl ThreadLocalAllocator {
    /// Create a new thread-local allocator
    pub fn new() -> Self {
        Self {
            pool: RefCell::new(MemoryPool::new()),
            logger: get_logger(),
        }
    }
    
    /// Get allocation statistics
    pub fn get_stats(&self) -> AllocStats {
        self.pool.borrow().get_stats()
    }
    
    /// Trim the memory pool
    pub fn trim(&self, threshold: usize) -> usize {
        self.pool.borrow().trim(threshold)
    }
    
    /// Release all cached memory
    pub fn release_all(&self) -> usize {
        self.pool.borrow().release_all()
    }
}

thread_local! {
    static THREAD_ALLOC: ThreadLocalAllocator = ThreadLocalAllocator::new();
}

/// A global allocator that uses thread-local memory pools
pub struct ThreadLocalPoolAllocator;

unsafe impl GlobalAlloc for ThreadLocalPoolAllocator {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        THREAD_ALLOC.with(|alloc| {
            let pool = alloc.pool.borrow();
            pool.allocate(layout)
        })
    }
    
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        THREAD_ALLOC.with(|alloc| {
            let pool = alloc.pool.borrow();
            pool.deallocate(ptr, layout)
        })
    }
}

/// Allocator API for Python
pub mod python_api {
    use super::*;
    use pyo3::prelude::*;
    
    /// A memory allocator for Python
    #[pyclass]
    pub struct PyMemoryAllocator {
        /// The underlying allocator
        pool: Arc<MemoryPool>,
    }
    
    #[pymethods]
    impl PyMemoryAllocator {
        /// Create a new memory allocator
        #[new]
        fn new() -> Self {
            Self {
                pool: Arc::new(MemoryPool::new()),
            }
        }
        
        /// Get allocation statistics
        fn get_stats(&self) -> PyResult<HashMap<String, PyObject>> {
            Python::with_gil(|py| {
                let stats = self.pool.get_stats();
                let mut result = HashMap::new();
                
                result.insert("total_allocations".to_string(), stats.total_allocations.into_py(py));
                result.insert("total_deallocations".to_string(), stats.total_deallocations.into_py(py));
                result.insert("current_allocations".to_string(), stats.current_allocations.into_py(py));
                result.insert("peak_allocations".to_string(), stats.peak_allocations.into_py(py));
                result.insert("total_bytes_allocated".to_string(), stats.total_bytes_allocated.into_py(py));
                result.insert("total_bytes_deallocated".to_string(), stats.total_bytes_deallocated.into_py(py));
                result.insert("current_bytes_allocated".to_string(), stats.current_bytes_allocated.into_py(py));
                result.insert("peak_bytes_allocated".to_string(), stats.peak_bytes_allocated.into_py(py));
                result.insert("cache_hits".to_string(), stats.cache_hits.into_py(py));
                result.insert("cache_misses".to_string(), stats.cache_misses.into_py(py));
                result.insert("reallocations".to_string(), stats.reallocations.into_py(py));
                result.insert("allocation_failures".to_string(), stats.allocation_failures.into_py(py));
                result.insert("largest_allocation".to_string(), stats.largest_allocation.into_py(py));
                result.insert("smallest_allocation".to_string(), stats.smallest_allocation.into_py(py));
                result.insert("average_allocation_size".to_string(), stats.average_allocation_size.into_py(py));
                result.insert("cache_hit_rate".to_string(), stats.cache_hit_rate().into_py(py));
                result.insert("allocation_success_rate".to_string(), stats.allocation_success_rate().into_py(py));
                result.insert("memory_utilization".to_string(), stats.memory_utilization().into_py(py));
                result.insert("fragmentation_ratio".to_string(), stats.fragmentation_ratio().into_py(py));
                
                Ok(result)
            })
        }
        
        /// Trim the memory pool
        fn trim(&self, threshold: usize) -> PyResult<usize> {
            Ok(self.pool.trim(threshold))
        }
        
        /// Release all cached memory
        fn release_all(&self) -> PyResult<usize> {
            Ok(self.pool.release_all())
        }
        
        /// Get a PyTorch allocator (if PyTorch is available)
        fn get_pytorch_allocator(&self) -> PyResult<PyObject> {
            Python::with_gil(|py| {
                let torch = py.import("torch")?;
                let allocator_class = torch.getattr("PyMemoryPoolAllocator")?;
                
                // Initialize PyTorch with our allocator
                // This is conceptual - implementing a custom PyTorch allocator
                // would require deeper integration
                let py_allocator = allocator_class.call0()?;
                
                Ok(py_allocator.into())
            })
        }
    }
}

#[tauri::command]
fn create_profile(
    state: State<AppState>,
    name: String,
    description: String,
    models: Vec<String>,
    frameworks: Vec<String>,
    hardware_targets: Vec<String>,
    batch_sizes: Vec<u32>,
    output_path: String,
) -> Result<Profile, String> {
    // Validate inputs
    if name.is_empty() {
        return Err("Profile name cannot be empty".to_string());
    }
    
    // Create output directory if it doesn't exist
    if !output_path.is_empty() && !Path::new(&output_path).exists() {
        fs::create_dir_all(&output_path)
            .map_err(|e| format!("Failed to create output directory: {}", e))?;
    }
    
    // Create new profile
    let now = chrono::Local::now().to_rfc3339();
    let profile = Profile {
        id: uuid::Uuid::new_v4().to_string(),
        name,
        description,
        created_at: now.clone(),
        updated_at: now,
        config: ProfileConfig {
            models,
            frameworks,
            hardware_targets,
            batch_sizes,
            output_path,
        },
    };
    
    // Update state
    {
        let mut profiles = state.profiles.lock().unwrap();
        profiles.push(profile.clone());
    }
    
    Ok(profile)
}

#[tauri::command]
fn set_current_profile(state: State<AppState>, id: String) -> Result<(), String> {
    let profiles = state.profiles.lock().unwrap();
    if !profiles.iter().any(|p| p.id == id) {
        return Err(format!("Profile with id '{}' not found", id));
    }
    
    let mut current_id = state.current_profile.lock().unwrap();
    *current_id = Some(id);
    
    Ok(())
}

#[tauri::command]
fn get_benchmarks(state: State<AppState>, profile_id: Option<String>) -> Result<Vec<Benchmark>, String> {
    let benchmarks = state.benchmarks.lock().unwrap();
    
    if let Some(pid) = profile_id {
        Ok(benchmarks.iter().filter(|b| b.profile_id == pid).cloned().collect())
    } else {
        Ok(benchmarks.clone())
    }
}

#[tauri::command]
fn create_benchmark(
    state: State<AppState>,
    profile_id: String,
    name: String,
    description: String,
) -> Result<Benchmark, String> {
    // Validate inputs
    if name.is_empty() {
        return Err("Benchmark name cannot be empty".to_string());
    }
    
    let profiles = state.profiles.lock().unwrap();
    if !profiles.iter().any(|p| p.id == profile_id) {
        return Err(format!("Profile with id '{}' not found", profile_id));
    }
    
    // Create new benchmark
    let now = chrono::Local::now().to_rfc3339();
    let benchmark = Benchmark {
        id: uuid::Uuid::new_v4().to_string(),
        profile_id,
        name,
        description,
        status: "created".to_string(),
        created_at: now,
        results: None,
    };
    
    // Update state
    {
        let mut benchmarks = state.benchmarks.lock().unwrap();
        benchmarks.push(benchmark.clone());
    }
    
    Ok(benchmark)
}

#[tauri::command]
fn run_benchmark(state: State<AppState>, benchmark_id: String) -> Result<(), String> {
    // Find benchmark
    let benchmark = {
        let mut benchmarks = state.benchmarks.lock().unwrap();
        let benchmark = benchmarks.iter_mut().find(|b| b.id == benchmark_id);
        
        if let Some(benchmark) = benchmark {
            benchmark.status = "running".to_string();
            benchmark.clone()
        } else {
            return Err(format!("Benchmark with id '{}' not found", benchmark_id));
        }
    };
    
    // Find profile
    let profile = {
        let profiles = state.profiles.lock().unwrap();
        profiles
            .iter()
            .find(|p| p.id == benchmark.profile_id)
            .cloned()
            .ok_or_else(|| format!("Profile with id '{}' not found", benchmark.profile_id))?
    };
    
    // Launch Python process to run the benchmark
    std::thread::spawn(move || {
        let output = Command::new("python")
            .arg("-m")
            .arg("mlperf.benchmarks.run_benchmark")
            .arg("--benchmark-id")
            .arg(&benchmark.id)
            .arg("--profile-id")
            .arg(&profile.id)
            .arg("--output-path")
            .arg(&profile.config.output_path)
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .output();
            
        match output {
            Ok(output) => {
                let stdout = String::from_utf8_lossy(&output.stdout);
                let stderr = String::from_utf8_lossy(&output.stderr);
                
                println!("Benchmark stdout: {}", stdout);
                println!("Benchmark stderr: {}", stderr);
                
                // Update benchmark status
                // In a real implementation, we'd parse the output and update the results
            },
            Err(e) => {
                eprintln!("Failed to run benchmark: {}", e);
            }
        }
    });
    
    Ok(())
}

#[tauri::command]
fn get_hardware_info() -> Result<serde_json::Value, String> {
    // Call Python script to get hardware info
    let output = Command::new("python")
        .arg("-m")
        .arg("mlperf.hardware.info")
        .output()
        .map_err(|e| format!("Failed to get hardware info: {}", e))?;
    
    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Hardware info command failed: {}", stderr));
    }
    
    let stdout = String::from_utf8_lossy(&output.stdout);
    let hardware_info: serde_json::Value = serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse hardware info: {}", e))?;
    
    Ok(hardware_info)
}

#[tauri::command]
fn get_framework_versions() -> Result<serde_json::Value, String> {
    // Call Python script to get framework versions
    let output = Command::new("python")
        .arg("-m")
        .arg("mlperf.utils.framework_versions")
        .output()
        .map_err(|e| format!("Failed to get framework versions: {}", e))?;
    
    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Framework versions command failed: {}", stderr));
    }
    
    let stdout = String::from_utf8_lossy(&output.stdout);
    let versions: serde_json::Value = serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse framework versions: {}", e))?;
    
    Ok(versions)
}

fn main() {
    tauri::Builder::default()
        .plugin(
            LoggerBuilder::new()
                .targets([LogTarget::LogDir, LogTarget::Stdout, LogTarget::Webview])
                .build(),
        )
        .manage(AppState::default())
        .invoke_handler(tauri::generate_handler![
            get_profiles,
            get_current_profile,
            create_profile,
            set_current_profile,
            get_benchmarks,
            create_benchmark,
            run_benchmark,
            get_hardware_info,
            get_framework_versions,
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}

const { TabPane } = Tabs;
const { Option } = Select;

function BenchmarkRunner({ onRefresh, onOpenFolderDialog }) {
  const { id: profileId } = useParams();
  const [profile, setProfile] = useState(null);
  const [benchmarks, setBenchmarks] = useState([]);
  const [loading, setLoading] = useState(true);
  const [runningBenchmark, setRunningBenchmark] = useState(false);
  const [benchmarkForm] = Form.useForm();
  const [selectedModels, setSelectedModels] = useState([]);
  const [selectedFrameworks, setSelectedFrameworks] = useState([]);
  const [selectedBatchSizes, setSelectedBatchSizes] = useState([]);
  const [selectedPrecision, setSelectedPrecision] = useState('fp32');
  const [enableDistributed, setEnableDistributed] = useState(false);
  const [numNodes, setNumNodes] = useState(1);

  useEffect(() => {
    loadProfileData();
  }, [profileId]);

  const loadProfileData = async () => {
    try {
      setLoading(true);
      
      // Load profile details
      const profileData = await invoke('get_current_profile');
      setProfile(profileData);
      
      if (profileData && profileData.config) {
        // Set default selections
        setSelectedModels(profileData.config.models.slice(0, 1));
        setSelectedFrameworks(profileData.config.frameworks.slice(0, 1));
        setSelectedBatchSizes(profileData.config.batch_sizes.slice(0, 1));
      }
      
      // Load benchmarks
      const benchmarksData = await invoke('get_benchmarks', { profileId });
      setBenchmarks(benchmarksData);
      
      setLoading(false);
    } catch (error) {
      console.error('Failed to load profile data:', error);
      message.error('Failed to load profile data');
      setLoading(false);
    }
  };

  const handleCreateBenchmark = async (values) => {
    try {
      const newBenchmark = await invoke('create_benchmark', {
        profileId,
        name: values.name,
        description: values.description || '',
      });
      
      message.success(`Benchmark "${values.name}" created successfully`);
      await loadProfileData();
      
      // Reset form
      benchmarkForm.resetFields();
      
      return newBenchmark;
    } catch (error) {
      console.error('Failed to create benchmark:', error);
      message.error('Failed to create benchmark');
    }
  };

  const handleRunBenchmark = async (benchmarkId) => {
    try {
      setRunningBenchmark(benchmarkId);
      
      await invoke('run_benchmark', { benchmarkId });
      
      message.success('Benchmark started successfully');
      
      // Poll for benchmark completion
      const checkBenchmarkStatus = async () => {
        const benchmarkData = await invoke('get_benchmarks', { profileId });
        const benchmark = benchmarkData.find(b => b.id === benchmarkId);
        
        if (benchmark && benchmark.status === 'completed') {
          message.success('Benchmark completed');
          await loadProfileData();
          setRunningBenchmark(null);
        } else if (benchmark && benchmark.status === 'failed') {
          message.error('Benchmark failed');
          await loadProfileData();
          setRunningBenchmark(null);
        } else {
          // Continue polling
          setTimeout(checkBenchmarkStatus, 5000);
        }
      };
      
      setTimeout(checkBenchmarkStatus, 5000);
      
    } catch (error) {
      console.error('Failed to run benchmark:', error);
      message.error('Failed to run benchmark');
      setRunningBenchmark(null);
    }
  };

  const handleRunSelected = async () => {
    try {
      if (!benchmarkForm.getFieldValue('name')) {
        // Generate a name based on selections
        const modelName = selectedModels.length === 1 ? selectedModels[0] : 'multi-model';
        const frameworkName = selectedFrameworks.length === 1 ? selectedFrameworks[0] : 'multi-framework';
        const batchSize = selectedBatchSizes.length === 1 ? selectedBatchSizes[0] : 'multi-batch';
        
        benchmarkForm.setFieldsValue({
          name: `${modelName}-${frameworkName}-${batchSize}-${selectedPrecision}`,
          description: `Benchmark for ${selectedModels.join(', ')} using ${selectedFrameworks.join(', ')} with batch sizes ${selectedBatchSizes.join(', ')} and ${selectedPrecision} precision.${enableDistributed ? ` Distributed across ${numNodes} nodes.` : ''}`
        });
      }
      
      // Submit the form to create the benchmark
      benchmarkForm.submit();
    } catch (error) {
      console.error('Failed to prepare benchmark:', error);
      message.error('Failed to prepare benchmark');
    }
  };

  const columns = [
    {
      title: 'Name',
      dataIndex: 'name',
      key: 'name',
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: (status) => {
        let color = 'default';
        if (status === 'running') color = 'processing';
        if (status === 'completed') color = 'success';
        if (status === 'failed') color = 'error';
        
        return <Tag color={color}>{status.toUpperCase()}</Tag>;
      },
    },
    {
      title: 'Created',
      dataIndex: 'created_at',
      key: 'created_at',
      render: (date) => new Date(date).toLocaleString(),
    },
    {
      title: 'Actions',
      key: 'actions',
      render: (_, record) => (
        <Space>
          {record.status !== 'running' && (
            <Button
              type="primary"
              icon={<PlayCircleOutlined />}
              loading={runningBenchmark === record.id}
              onClick={() => handleRunBenchmark(record.id)}
              disabled={runningBenchmark !== null}
            >
              Run
            </Button>
          )}
          
          {record.status === 'completed' && (
            <Button
              type="default"
              icon={<BarChartOutlined />}
              onClick={() => window.location.href = `/benchmark/${record.id}/results`}
            >
              Results
            </Button>
          )}
        </Space>
      ),
    },
  ];

  if (loading) {
    return <Spin tip="Loading..." size="large" className="loading-spinner" />;
  }

  return (
    <div className="benchmark-runner">
      <h1>Benchmark Runner</h1>
      {profile && (
        <p>Profile: <strong>{profile.name}</strong> - {profile.description}</p>
      )}
      
      <Tabs defaultActiveKey="new">
        <TabPane tab="New Benchmark" key="new" className="tab-content">
          <div className="benchmark-config">
            <div className="config-section">
              <h3><ExperimentOutlined /> Models & Frameworks</h3>
              <div className="selection-row">
                <div className="selection-item">
                  <label>Models:</label>
                  <ModelSelector
                    models={profile?.config?.models || []}
                    selectedModels={selectedModels}
                    onChange={setSelectedModels}
                  />
                </div>
                
                <div className="selection-item">
                  <label>Frameworks:</label>
                  <Select
                    mode="multiple"
                    style={{ width: '100%' }}
                    placeholder="Select frameworks"
                    value={selectedFrameworks}
                    onChange={setSelectedFrameworks}
                  >
                    {profile?.config?.frameworks.map(framework => (
                      <Option key={framework} value={framework}>{framework}</Option>
                    ))}
                  </Select>
                </div>
              </div>
            </div>
            
            <div className="config-section">
              <h3><SettingOutlined /> Configuration</h3>
              <div className="selection-row">
                <div className="selection-item">
                  <label>Batch Sizes:</label>
                  <BatchSizeSelector
                    batchSizes={profile?.config?.batch_sizes || []}
                    selectedBatchSizes={selectedBatchSizes}
                    onChange={setSelectedBatchSizes}
                  />
                </div>
                
                <div className="selection-item">
                  <label>Precision:</label>
                  <Select
                    style={{ width: '100%' }}
                    value={selectedPrecision}
                    onChange={setSelectedPrecision}
                  >
                    <Option value="fp32">FP32</Option>
                    <Option value="fp16">FP16</Option>
                    <Option value="bf16">BF16</Option>
                    <Option value="int8">INT8</Option>
                  </Select>
                </div>
              </div>
            </div>
            
            <div className="config-section">
              <h3><ThunderboltOutlined /> Advanced Options</h3>
              <div className="selection-row">
                <div className="selection-item">
                  <label>Distributed Training:</label>
                  <Switch 
                    checked={enableDistributed}
                    onChange={setEnableDistributed}
                  />
                </div>
                
                {enableDistributed && (
                  <div className="selection-item">
                    <label>Number of Nodes:</label>
                    <Slider
                      min={1}
                      max={8}
                      value={numNodes}
                      onChange={setNumNodes}
                      marks={{ 1: '1', 2: '2', 4: '4', 8: '8' }}
                    />
                  </div>
                )}
              </div>
            </div>
            
            <Divider />
            
            <Form
              form={benchmarkForm}
              layout="vertical"
              onFinish={handleCreateBenchmark}
            >
              <Form.Item
                name="name"
                label="Benchmark Name"
                rules={[{ required: true, message: 'Please enter a name' }]}
              >
                <Input placeholder="Enter benchmark name" />
              </Form.Item>
              
              <Form.Item
                name="description"
                label="Description"
              >
                <Input.TextArea placeholder="Enter benchmark description" rows={3} />
              </Form.Item>
              
              <Form.Item>
                <Button
                  type="primary"
                  icon={<PlayCircleOutlined />}
                  onClick={handleRunSelected}
                  disabled={
                    selectedModels.length === 0 || 
                    selectedFrameworks.length === 0 || 
                    selectedBatchSizes.length === 0 ||
                    runningBenchmark !== null
                  }
                >
                  Run Benchmark
                </Button>
              </Form.Item>
            </Form>
          </div>
        </TabPane>
        
        <TabPane tab="Benchmark History" key="history" className="tab-content">
          <Table
            dataSource={benchmarks}
            columns={columns}
            rowKey="id"
          />
        </TabPane>
        
        <TabPane tab="Hardware Info" key="hardware" className="tab-content">
          <div className="hardware-info">
            <HardwareCard />
          </div>
        </TabPane>
      </Tabs>
    </div>
  );
}

export default BenchmarkRunner;
pytest-xdist = "^3.3.1"
openai = "^1.20.0"

[tool.poetry.dev-dependencies]
black = "^23.3.0"
isort = "^5.12.0"
mypy = "^1.3.0"
flake8 = "^6.0.0"
sphinx = "^7.0.0"
sphinx-rtd-theme = "^1.2.0"
jupyterlab = "^4.0.0"
pytest-mock = "^3.10.0"

[tool.poetry.scripts]
mlperf = "mlperf.cli:app"

[build-system]
requires = ["poetry-core>=1.0.0", "maturin>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.black]
line-length = 88
target-version = ["py38"]

[tool.isort]
profile = "black"
line_length = 88

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true

[tool.pytest.ini_options]
testpaths = ["python/tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "--cov=mlperf --cov-report=xml --cov-report=term"

